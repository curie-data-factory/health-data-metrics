{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Qu'est-ce que HDM ? Health data metrics (HDM) est un outil d\u00e9velopp\u00e9 par la Data Factory de l'Institut Curie . Le but Son but est de pouvoir calculer des m\u00e9triques de qualit\u00e9 sur de la donn\u00e9e m\u00e9dicale et son stockage dans les entrep\u00f4ts de donn\u00e9es. Le but premier est de pouvoir am\u00e9liorer et monitorer la qualit\u00e9 des donn\u00e9es de nos entrep\u00f4ts de donn\u00e9e de sant\u00e9. Pour ce faire nous avons d\u00e9velopp\u00e9 les fonctionnalit\u00e9s suivantes : Calculer des m\u00e9triques sur les donn\u00e9es de nos entrep\u00f4ts. Mettre en place des r\u00e8gles pour pouvoir appliquer des contraintes op\u00e9rationnelles/m\u00e9tiers sur les bases de donn\u00e9es en lien avec les m\u00e9triques calcul\u00e9es. D\u00e9tecter les ruptures et r\u00e9gressions dans la structure de la base de donn\u00e9es ou dans les donn\u00e9es elles-m\u00eames en g\u00e9n\u00e9rant des alertes gr\u00e2ce \u00e0 des r\u00e8gles m\u00e9tiers. Permettre de centraliser les contraintes et cr\u00e9er un HUB unifi\u00e9 pour g\u00e9rer la qualit\u00e9 de la donn\u00e9e afin de livrer des donn\u00e9es de la meilleure qualit\u00e9 possible aux m\u00e9decins et chercheurs. Cr\u00e9er des dashboards sur les m\u00e9triques pour pouvoir les visualiser et les explorer . Les principes de base Il existe une relation hi\u00e9rarchique dans la fa\u00e7on dont HDM fonctionne : Premi\u00e8rement, nous avons des bases de donn\u00e9es \u00e0 sur lesquelles nous allons calculer des m\u00e9triques Ces m\u00e9triques peuvent servir diff\u00e9rents buts et sont calcul\u00e9es gr\u00e2ce \u00e0 des M\u00e9triques Packs qui sont des mini programmes autonomes qui vont se charger de les calculer puis de les ins\u00e9rer dans une base de donn\u00e9es de m\u00e9trique . Ensuite nous avons les Rules Packs dont le but est de pouvoir exploiter les m\u00e9triques calcul\u00e9es pr\u00e9c\u00e9demment afin de g\u00e9n\u00e9rer des alertes que l'utilisateur pourra consulter. De la m\u00eame mani\u00e8re que pour les m\u00e9triques Packs, les Rule Packs fonctionnent \u00e9galement comme des mini programmes autonomes qui vont se charger cette fois de g\u00e9n\u00e9rer des r\u00e8gles bas\u00e9es sur les m\u00e9triques et de g\u00e9n\u00e9rer des alertes si jamais ces r\u00e8gles ne sont pas respect\u00e9es . L'architecture Fonctionnelle HDM fonctionne en articulation de plusieurs outils et technologies que nous allons voir dans la suite : Logiciel Containerized Version Usage url Elasticsearch Yes 7.10.0 Permet de stocker les m\u00e9triques sous forme de documents pour la construction des dashboards https://www.elastic.co/fr/products/elasticsearch Kibana Yes 7.10.0 Permet de r\u00e9aliser des tableaux de visualisations interactives pour l'analyse exploratoire des m\u00e9triques https://www.elastic.co/fr/products/kibana Python Yes 3.6.8 Permet d'ex\u00e9cuter les scripts pour le calcul des m\u00e9triques et la g\u00e9n\u00e9ration des alertes https://www.python.org/download/releases/3.0/ Apache Yes php:7.4.5-apache Serveur Web avec PHP 7 pour l'affichage de la web-Application https://hub.docker.com/ MySQL Yes 8.0.16 Base de donn\u00e9es relationnelle qui permet de stocker les m\u00e9triques, les r\u00e8gles et les alertes. https://www.mysql.com/fr/ Airflow Yes 2.1.0 Orchestrateur de T\u00e2ches. https://airflow.apache.org/ Les composants Bases de donn\u00e9es Cet outil int\u00e8gre 3 bases de donn\u00e9es : Une base MySQL pour stocker les m\u00e9triques sous format SQL. Une base MySQL pour stocker les donn\u00e9es applicatives. (conf , param\u00e8tres, logs etc...) Une base Elasticsearch pour stocker les m\u00e9triques sous format NO-SQL. Modules Front-end L'outil de front-end fonctionne de fa\u00e7on modulaire, il existe les modules suivants : Explorer (Dashboarding pour explorer les m\u00e9triques) Rule editor (Permet d'\u00e9diter les r\u00e8gles de coh\u00e9rence directement dans l'interface sans lignes de codes) Alerts (Permet d'afficher les alertes remont\u00e9s par les Rule Packs) Admin (Permet de g\u00e9rer l'administration des MP/RP et leur configuration) Dashboards Cet outil peut \u00eatre utilis\u00e9 avec les outils suivants : Kibana qui permet de cr\u00e9er des vues \u00e0 partir des documents dans les bases NO-SQL Elasticsearch. Redash qui permet de cr\u00e9er des vues \u00e0 partir des tables de m\u00e9triques SQL MySQL. Orchestrateur & Jobs Afin de monitorer l'ex\u00e9cution des t\u00e2ches de calcul de m\u00e9triques, de r\u00e8gles et d'alertes, nous utilisons Airflow, \u00e0 travers la d\u00e9finition d'un DAG appel\u00e9 : hdm-pipeline . M\u00e9triques et r\u00e8gles Comment sont d\u00e9fini et calcul\u00e9s les modules de m\u00e9triques et de r\u00e8gles ? Dans HDM il existe ce qu'on appelle des \"PACK\", les packs sont des mini-modules qui se chargent de calculer soit des m\u00e9triques, soit des r\u00e8gles de coh\u00e9rences de donn\u00e9es. Dans la version actuelle de HDM il n'existe qu'un seul pack de m\u00e9triques mais il est possible d'en rajouter d'autres par la suite sans avoir \u00e0 modifier l'applicatif. Fonctionnement des \"MetricPacks\" (MP) Les m\u00e9triques packs contiennent un fichier .sh qui contient la ligne d'ex\u00e9cution du MP, il est \u00e0 la charge du programmeur du MP de faire toutes les v\u00e9rifications de d\u00e9pendances logicielles avant d'ex\u00e9cuter son programme. Le metricPack peut \u00e9galement contenir un fichier .ndjson contenant les visualisation & dashboards Kibana qui sont li\u00e9s aux donn\u00e9es de m\u00e9triques que produisent son MP. Fonctionnement des \"RulePacks\" (RP) Les rules packs contiennent un fichier .sh qui contient la ligne d'ex\u00e9cution du RP, il est \u00e0 la charge du programmeur du MP de faire toutes les v\u00e9rifications de d\u00e9pendances logicielles avant d'ex\u00e9cuter son programme. Les rules packs s'ex\u00e9cutent sur les donn\u00e9es des MP et ajoutes des alertes dans la table d'alerte de HDM. Configuration & Orchestration La configuration de l'applicatif HDM est interne \u00e0 l'applicatif lui-m\u00eame, il existe les configurations suivantes : Configuration LDAP : Permet de se connecter au serveur LDAP, g\u00e8re l'authentification des utilisateurs et la gestion de leurs droits Configuration Application : Quels dashboards Kibana afficher dans quels modules etc... Configuration Packs : Connexion au r\u00e9pertoire de Packs (Nexus) Configuration g\u00e9n\u00e9rale des packs Chaque MetricPack & RulePack peux poss\u00e9der sa configuration propre. Elle est stock\u00e9e en base64 dans la base de donn\u00e9es applicative. Chaque MP & RP peut \u00e9galement poss\u00e9der sa configuration propre par base de donn\u00e9es scann\u00e9e et peut donc adapter son fonctionnement \u00e0 la base de donn\u00e9es cible si besoin. Si cette configuration existe, elle sera charg\u00e9e en lieu et place de la configuration du MP/RP parente. L'orchestration est effectu\u00e9e par un DAG (Directed Acyclic Graph) Airflow qui se chargera de charger les MP/RP puis, de charger leur configuration, et enfin les ex\u00e9cutera sur chaque base de donn\u00e9es. Les m\u00e9triques de qualit\u00e9s de la donn\u00e9e Les niveaux de m\u00e9triques Afin d'\u00e9valuer de fa\u00e7on correcte la qualit\u00e9 des donn\u00e9es, il est bon de d\u00e9finir des p\u00e9rim\u00e8tres de calcul de ces m\u00e9triques. Nous avons d\u00e9fini 6 niveaux hi\u00e9rarchiques de m\u00e9triques; Il est important de ne pas m\u00e9langer les niveaux de m\u00e9triques, et de comparer les m\u00e9triques entres-elles sur le m\u00eame niveau. Le niveau 0 : Le niveau 0 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle de toutes les versions d'une base de donn\u00e9es. Le niveau 1 : Le niveau 1 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une version donn\u00e9e d'une base de donn\u00e9es. Le niveau 2 : Le niveau 2 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une table, d'une version donn\u00e9e, d'une base de donn\u00e9es. Le niveau 3 : Le niveau 3 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une colonne ind\u00e9pendamment de son type (exemple : nombres de valeurs manquantes/valeurs NULL). Le niveau 4 : Le niveau 4 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une colonne en prenant en compte son type de donn\u00e9e (Num\u00e9rique, textuelle, cat\u00e9gorique, continue, date, id etc. ...). Le niveau 5 : Le niveau 5 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle de variables cat\u00e9goriques : (fr\u00e9quence, valeur) Sch\u00e9ma de repr\u00e9sentation des diff\u00e9rents niveaux de calcul de m\u00e9triques possible. Les r\u00e8gles & alertes Les r\u00e8gles Bas\u00e9 sur les m\u00e9triques calcul\u00e9es p\u00e9riodiquement, l'application permet d'assurer un suivi passif (sans intervention humaine) et dynamique (s'adapte aux changements) de la qualit\u00e9 de la donn\u00e9e, \u00e0 l'aide de l'outil d'impl\u00e9mentation de r\u00e8gles d'alertes afin de remonter le plus efficacement les anomalies dans la base de donn\u00e9es. Affichage de l'interface graphique de l'\u00e9diteur de r\u00e8gles. Les Alertes Les alertes seront pr\u00e9sent\u00e9s sous forme de liste simple et organis\u00e9s par base de donn\u00e9es, table ou colonnes. Affichage de la table des alertes. La visualisation et l'exploration La visualisation des m\u00e9triques et leur exploration est une phase importante afin de construire les r\u00e8gles les plus pertinentes possibles. Un outil est \u00e0 disposition appel\u00e9 Kibana, bas\u00e9 sur la base de donn\u00e9es Elasticsearch. Les dashboard permettrons de visualiser et d'interagir simplement avec les m\u00e9triques afin de pouvoir d\u00e9tecter les valeurs aberrantes facilement, et de pouvoir faire remonter ces anomalies en alerte gr\u00e2ce \u00e0 l'\u00e9diteur de r\u00e8gles. Affichage de l'interface graphique de dashboard kibana.","title":"Home"},{"location":"index.html#quest-ce-que-hdm","text":"Health data metrics (HDM) est un outil d\u00e9velopp\u00e9 par la Data Factory de l'Institut Curie .","title":"Qu'est-ce que HDM ?"},{"location":"index.html#le-but","text":"Son but est de pouvoir calculer des m\u00e9triques de qualit\u00e9 sur de la donn\u00e9e m\u00e9dicale et son stockage dans les entrep\u00f4ts de donn\u00e9es. Le but premier est de pouvoir am\u00e9liorer et monitorer la qualit\u00e9 des donn\u00e9es de nos entrep\u00f4ts de donn\u00e9e de sant\u00e9. Pour ce faire nous avons d\u00e9velopp\u00e9 les fonctionnalit\u00e9s suivantes : Calculer des m\u00e9triques sur les donn\u00e9es de nos entrep\u00f4ts. Mettre en place des r\u00e8gles pour pouvoir appliquer des contraintes op\u00e9rationnelles/m\u00e9tiers sur les bases de donn\u00e9es en lien avec les m\u00e9triques calcul\u00e9es. D\u00e9tecter les ruptures et r\u00e9gressions dans la structure de la base de donn\u00e9es ou dans les donn\u00e9es elles-m\u00eames en g\u00e9n\u00e9rant des alertes gr\u00e2ce \u00e0 des r\u00e8gles m\u00e9tiers. Permettre de centraliser les contraintes et cr\u00e9er un HUB unifi\u00e9 pour g\u00e9rer la qualit\u00e9 de la donn\u00e9e afin de livrer des donn\u00e9es de la meilleure qualit\u00e9 possible aux m\u00e9decins et chercheurs. Cr\u00e9er des dashboards sur les m\u00e9triques pour pouvoir les visualiser et les explorer .","title":"Le but"},{"location":"index.html#les-principes-de-base","text":"Il existe une relation hi\u00e9rarchique dans la fa\u00e7on dont HDM fonctionne : Premi\u00e8rement, nous avons des bases de donn\u00e9es \u00e0 sur lesquelles nous allons calculer des m\u00e9triques Ces m\u00e9triques peuvent servir diff\u00e9rents buts et sont calcul\u00e9es gr\u00e2ce \u00e0 des M\u00e9triques Packs qui sont des mini programmes autonomes qui vont se charger de les calculer puis de les ins\u00e9rer dans une base de donn\u00e9es de m\u00e9trique . Ensuite nous avons les Rules Packs dont le but est de pouvoir exploiter les m\u00e9triques calcul\u00e9es pr\u00e9c\u00e9demment afin de g\u00e9n\u00e9rer des alertes que l'utilisateur pourra consulter. De la m\u00eame mani\u00e8re que pour les m\u00e9triques Packs, les Rule Packs fonctionnent \u00e9galement comme des mini programmes autonomes qui vont se charger cette fois de g\u00e9n\u00e9rer des r\u00e8gles bas\u00e9es sur les m\u00e9triques et de g\u00e9n\u00e9rer des alertes si jamais ces r\u00e8gles ne sont pas respect\u00e9es .","title":"Les principes de base"},{"location":"index.html#larchitecture-fonctionnelle","text":"HDM fonctionne en articulation de plusieurs outils et technologies que nous allons voir dans la suite : Logiciel Containerized Version Usage url Elasticsearch Yes 7.10.0 Permet de stocker les m\u00e9triques sous forme de documents pour la construction des dashboards https://www.elastic.co/fr/products/elasticsearch Kibana Yes 7.10.0 Permet de r\u00e9aliser des tableaux de visualisations interactives pour l'analyse exploratoire des m\u00e9triques https://www.elastic.co/fr/products/kibana Python Yes 3.6.8 Permet d'ex\u00e9cuter les scripts pour le calcul des m\u00e9triques et la g\u00e9n\u00e9ration des alertes https://www.python.org/download/releases/3.0/ Apache Yes php:7.4.5-apache Serveur Web avec PHP 7 pour l'affichage de la web-Application https://hub.docker.com/ MySQL Yes 8.0.16 Base de donn\u00e9es relationnelle qui permet de stocker les m\u00e9triques, les r\u00e8gles et les alertes. https://www.mysql.com/fr/ Airflow Yes 2.1.0 Orchestrateur de T\u00e2ches. https://airflow.apache.org/","title":"L'architecture Fonctionnelle"},{"location":"index.html#les-composants","text":"","title":"Les composants"},{"location":"index.html#bases-de-donnees","text":"Cet outil int\u00e8gre 3 bases de donn\u00e9es : Une base MySQL pour stocker les m\u00e9triques sous format SQL. Une base MySQL pour stocker les donn\u00e9es applicatives. (conf , param\u00e8tres, logs etc...) Une base Elasticsearch pour stocker les m\u00e9triques sous format NO-SQL.","title":"Bases de donn\u00e9es"},{"location":"index.html#modules-front-end","text":"L'outil de front-end fonctionne de fa\u00e7on modulaire, il existe les modules suivants : Explorer (Dashboarding pour explorer les m\u00e9triques) Rule editor (Permet d'\u00e9diter les r\u00e8gles de coh\u00e9rence directement dans l'interface sans lignes de codes) Alerts (Permet d'afficher les alertes remont\u00e9s par les Rule Packs) Admin (Permet de g\u00e9rer l'administration des MP/RP et leur configuration)","title":"Modules Front-end"},{"location":"index.html#dashboards","text":"Cet outil peut \u00eatre utilis\u00e9 avec les outils suivants : Kibana qui permet de cr\u00e9er des vues \u00e0 partir des documents dans les bases NO-SQL Elasticsearch. Redash qui permet de cr\u00e9er des vues \u00e0 partir des tables de m\u00e9triques SQL MySQL.","title":"Dashboards"},{"location":"index.html#orchestrateur-jobs","text":"Afin de monitorer l'ex\u00e9cution des t\u00e2ches de calcul de m\u00e9triques, de r\u00e8gles et d'alertes, nous utilisons Airflow, \u00e0 travers la d\u00e9finition d'un DAG appel\u00e9 : hdm-pipeline .","title":"Orchestrateur &amp; Jobs"},{"location":"index.html#metriques-et-regles","text":"Comment sont d\u00e9fini et calcul\u00e9s les modules de m\u00e9triques et de r\u00e8gles ? Dans HDM il existe ce qu'on appelle des \"PACK\", les packs sont des mini-modules qui se chargent de calculer soit des m\u00e9triques, soit des r\u00e8gles de coh\u00e9rences de donn\u00e9es. Dans la version actuelle de HDM il n'existe qu'un seul pack de m\u00e9triques mais il est possible d'en rajouter d'autres par la suite sans avoir \u00e0 modifier l'applicatif.","title":"M\u00e9triques et r\u00e8gles"},{"location":"index.html#fonctionnement-des-metricpacks-mp","text":"Les m\u00e9triques packs contiennent un fichier .sh qui contient la ligne d'ex\u00e9cution du MP, il est \u00e0 la charge du programmeur du MP de faire toutes les v\u00e9rifications de d\u00e9pendances logicielles avant d'ex\u00e9cuter son programme. Le metricPack peut \u00e9galement contenir un fichier .ndjson contenant les visualisation & dashboards Kibana qui sont li\u00e9s aux donn\u00e9es de m\u00e9triques que produisent son MP.","title":"Fonctionnement des \"MetricPacks\" (MP)"},{"location":"index.html#fonctionnement-des-rulepacks-rp","text":"Les rules packs contiennent un fichier .sh qui contient la ligne d'ex\u00e9cution du RP, il est \u00e0 la charge du programmeur du MP de faire toutes les v\u00e9rifications de d\u00e9pendances logicielles avant d'ex\u00e9cuter son programme. Les rules packs s'ex\u00e9cutent sur les donn\u00e9es des MP et ajoutes des alertes dans la table d'alerte de HDM.","title":"Fonctionnement des \"RulePacks\" (RP)"},{"location":"index.html#configuration-orchestration","text":"La configuration de l'applicatif HDM est interne \u00e0 l'applicatif lui-m\u00eame, il existe les configurations suivantes : Configuration LDAP : Permet de se connecter au serveur LDAP, g\u00e8re l'authentification des utilisateurs et la gestion de leurs droits Configuration Application : Quels dashboards Kibana afficher dans quels modules etc... Configuration Packs : Connexion au r\u00e9pertoire de Packs (Nexus) Configuration g\u00e9n\u00e9rale des packs Chaque MetricPack & RulePack peux poss\u00e9der sa configuration propre. Elle est stock\u00e9e en base64 dans la base de donn\u00e9es applicative. Chaque MP & RP peut \u00e9galement poss\u00e9der sa configuration propre par base de donn\u00e9es scann\u00e9e et peut donc adapter son fonctionnement \u00e0 la base de donn\u00e9es cible si besoin. Si cette configuration existe, elle sera charg\u00e9e en lieu et place de la configuration du MP/RP parente. L'orchestration est effectu\u00e9e par un DAG (Directed Acyclic Graph) Airflow qui se chargera de charger les MP/RP puis, de charger leur configuration, et enfin les ex\u00e9cutera sur chaque base de donn\u00e9es.","title":"Configuration &amp; Orchestration"},{"location":"index.html#les-metriques-de-qualites-de-la-donnee","text":"","title":"Les m\u00e9triques de qualit\u00e9s de la donn\u00e9e"},{"location":"index.html#les-niveaux-de-metriques","text":"Afin d'\u00e9valuer de fa\u00e7on correcte la qualit\u00e9 des donn\u00e9es, il est bon de d\u00e9finir des p\u00e9rim\u00e8tres de calcul de ces m\u00e9triques. Nous avons d\u00e9fini 6 niveaux hi\u00e9rarchiques de m\u00e9triques; Il est important de ne pas m\u00e9langer les niveaux de m\u00e9triques, et de comparer les m\u00e9triques entres-elles sur le m\u00eame niveau. Le niveau 0 : Le niveau 0 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle de toutes les versions d'une base de donn\u00e9es. Le niveau 1 : Le niveau 1 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une version donn\u00e9e d'une base de donn\u00e9es. Le niveau 2 : Le niveau 2 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une table, d'une version donn\u00e9e, d'une base de donn\u00e9es. Le niveau 3 : Le niveau 3 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une colonne ind\u00e9pendamment de son type (exemple : nombres de valeurs manquantes/valeurs NULL). Le niveau 4 : Le niveau 4 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle d'une colonne en prenant en compte son type de donn\u00e9e (Num\u00e9rique, textuelle, cat\u00e9gorique, continue, date, id etc. ...). Le niveau 5 : Le niveau 5 permet de calculer des m\u00e9triques \u00e0 l'\u00e9chelle de variables cat\u00e9goriques : (fr\u00e9quence, valeur) Sch\u00e9ma de repr\u00e9sentation des diff\u00e9rents niveaux de calcul de m\u00e9triques possible.","title":"Les niveaux de m\u00e9triques"},{"location":"index.html#les-regles-alertes","text":"","title":"Les r\u00e8gles &amp; alertes"},{"location":"index.html#les-regles","text":"Bas\u00e9 sur les m\u00e9triques calcul\u00e9es p\u00e9riodiquement, l'application permet d'assurer un suivi passif (sans intervention humaine) et dynamique (s'adapte aux changements) de la qualit\u00e9 de la donn\u00e9e, \u00e0 l'aide de l'outil d'impl\u00e9mentation de r\u00e8gles d'alertes afin de remonter le plus efficacement les anomalies dans la base de donn\u00e9es. Affichage de l'interface graphique de l'\u00e9diteur de r\u00e8gles.","title":"Les r\u00e8gles"},{"location":"index.html#les-alertes","text":"Les alertes seront pr\u00e9sent\u00e9s sous forme de liste simple et organis\u00e9s par base de donn\u00e9es, table ou colonnes. Affichage de la table des alertes.","title":"Les Alertes"},{"location":"index.html#la-visualisation-et-lexploration","text":"La visualisation des m\u00e9triques et leur exploration est une phase importante afin de construire les r\u00e8gles les plus pertinentes possibles. Un outil est \u00e0 disposition appel\u00e9 Kibana, bas\u00e9 sur la base de donn\u00e9es Elasticsearch. Les dashboard permettrons de visualiser et d'interagir simplement avec les m\u00e9triques afin de pouvoir d\u00e9tecter les valeurs aberrantes facilement, et de pouvoir faire remonter ces anomalies en alerte gr\u00e2ce \u00e0 l'\u00e9diteur de r\u00e8gles. Affichage de l'interface graphique de dashboard kibana.","title":"La visualisation et l'exploration"},{"location":"full-installation.html","text":"HDM Full Installation This Tutorial guides you on How to install hdm full stack on your local machine. \u26a0\ufe0f REQUIREMENTS \u26a0\ufe0f Software : Linux/MacOS 64bit or Windows 10 64bit with WSL2 Docker Docker-compose Python 3.9+ Hardware : Minimal \ud83e\udd13 - CPU : 4 Cores - RAM : 16 Go - Storage : 10 Go Hardware : Recommended \ud83d\ude0e - CPU : 12 Cores - RAM : 32 Go - Storage : 30 Go In this Tutorial, we are going to install HDM in Full Stack mode. That means that we are going to : \u26a0\ufe0f Before we start : all comandlines have to be executed at the root folder of the git source repository 1. Launch all the software stack : - Airflow - Nexus - Elasticsearch - Kibana - MySQL - HDM frontend 2. Ingest some dataset to our MySQL database, simulating a dataware that we want to scan. 3. Register our Metric Packs & Rule Packs on Nexus and configure them into HDM . 4. Add an Airflow DAG to run them. 5. Run our HDM Airflow DAG and compute metrics/alerts . 6. Finally, add our Kibana Dashboards and use the Explorer and Alert Dashboard . 1. Launch all the software stack To run the stack we need : Docker See Get Docker Docker Compose See Get Docker Compose We are going to run the docker-compose files : - docker-compose.yml (HDM primary Stack) - docker-compose-airflow.yml (Airflow Stack) More INFO Here bash tutorials/full-installation/launch-stack.sh When the installation is complete, you should check the different application endpoints : http://localhost:80 HDM http://localhost:8081 Nexus http://localhost:5601 Kibana http://localhost:8080 Airflow (User: airflow | Password: airflow) tcp://127.0.0.1:3306 MySQL Endpoint host : 127.0.0.1 | Port : 3306 | User : hdm | Password : password | Database : dbhdm or: host : 127.0.0.1 | Port : 3306 | User : root | Password : rootpassword When you have all done. Let's go to the next step. 2. Ingest a Dataset We are using the Kaggle API to download our example datasets. 2.1 Kaggle cli installation In a Client with python 3 on it, run : pip install kaggle --upgrade 2.2 Kaggle cli login Type kaggle to check if kaggle is installed. Setup API credentials : https://github.com/Kaggle/kaggle-api#api-credentials Run the commandline if needed : Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/<User>/.kaggle/kaggle.json' Test with : kaggle datasets list 2.3 Download Heart Attack Dataset kaggle datasets download rashikrahmanpritom/heart-attack-analysis-prediction-dataset -p ./datasets --unzip 2.4 Run Ingestion Script We are now going to ingest our Kaggle dataset to our MySQL database. bash tutorials/full-installation/ingest-data.sh Data is ingested ! Check it out on mysql://127.0.0.1:3306/heart-attack 3. Metric Pack & Rule Pack Registration In this step, we are going to register the metric pack and rule pack that are used for HDM. 3.1 Setup Nexus In order to setup Nexus we need to get the password : docker exec -ti nexus sh -c \"cat /nexus-data/admin.password\" This will give you the admin password for Nexus Go to http://localhost:8081/ and login as \"admin\" + [Password from previous command] Do the setup by changing the default admin password and then checkout the [x][Enable Anonymous Access] 3.2 Run Nexus Import Script \u26a0\ufe0f Change the PASSWORDNEXUS to your Nexus admin password value. # Nexus User Credentials export PASSWORDNEXUS = \"123qwe\" export USERNEXUS = \"admin\" And run the script : bash tutorials/full-installation/mp-rp-nexus-register.sh This script will create a Maven2 Repository on Nexus named : hdm-snapshots The script then packages into zip files the metric pack & rule pack basic and upload them into the maven repository. Check if it's ok : http://localhost:8081/#browse/browse:hdm-snapshots 3.3. HDM DB Initialization We now have to initialize the hdm core db Go to the [Databases] Admin Tab http://localhost/admin.php?tab=databases Then click in this order on : [Launch db hdm script creator] [Sync db-config File to HDM's Table [Database List]] 3.4. Metric Pack & Rule Pack Configuration 3.4.1 Enabling Metric Pack / Rule Pack We Then have to activate our mp & rp on : http://localhost/admin.php?tab=metricpacks http://localhost/admin.php?tab=rulepacks 3.4.2 Edit Configuration Metric Pack / Rule Pack We edit our metric pack configuration to add : { \"print_cat_var\" : false , \"print_mat_num_plot\" : false , \"limit_enabled\" : true , \"search_results_limit\" : 2000000 , \"rootResultFolder\" : \"../results/\" , \"esHost\" : \"elasticsearch\" , \"esPort\" : 9200 , \"esSSL\" : false } And same for our rule pack with : dev 4. Airflow DAG Login to Airflow http://localhost:8080/home with (login : airflow | password: airflow) 4.1 Add env variables In your previous terminal run these commands : # Airflow User Credentials export PASSWORDAIRFLOW = \"airflow\" export USERAIRFLOW = \"airflow\" # Add variables curl -u $USERAIRFLOW : $PASSWORDAIRFLOW -X POST \"http://localhost:8080/api/v1/variables\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"key\\\":\\\"env\\\",\\\"value\\\":\\\"dev\\\"}\" They will create all the airflow environment variables in order for our DAG to run. 4.2 Enable the dag Toggle the dag : 5. Run the dag Trigger the dag : You can check it's execution : http://localhost:8080/graph?dag_id=hdm-pipeline 6. HDM Visualisation 6.1 Import kibana dashboard Run the following comandline to import all the dashboards from the Basic Metric Pack into kibana. curl -X POST http://localhost:5601/api/saved_objects/_import?overwrite = true -H \"kbn-xsrf: true\" --form file = @packs/hdm-metric-packs/basic/kibana-dashboard/export.ndjson 6.2 Explorer Dashboard You can explore the different metric pack dashboards from the Explorer. http://localhost/explorer/wrapper.php 6.3 Alert Dashboard You can check all the alerts emmitted by the different rule packs from the Alert dashboard : http://localhost/alert/alert.php 7. Stopping HDM Stack To stop the stack : docker-compose -f docker-compose.yml down docker-compose -f docker-compose-airflow.yaml down -v docker-compose down -v","title":"HDM Full Installation"},{"location":"full-installation.html#hdm-full-installation","text":"This Tutorial guides you on How to install hdm full stack on your local machine. \u26a0\ufe0f REQUIREMENTS \u26a0\ufe0f Software : Linux/MacOS 64bit or Windows 10 64bit with WSL2 Docker Docker-compose Python 3.9+ Hardware : Minimal \ud83e\udd13 - CPU : 4 Cores - RAM : 16 Go - Storage : 10 Go Hardware : Recommended \ud83d\ude0e - CPU : 12 Cores - RAM : 32 Go - Storage : 30 Go In this Tutorial, we are going to install HDM in Full Stack mode. That means that we are going to : \u26a0\ufe0f Before we start : all comandlines have to be executed at the root folder of the git source repository 1. Launch all the software stack : - Airflow - Nexus - Elasticsearch - Kibana - MySQL - HDM frontend 2. Ingest some dataset to our MySQL database, simulating a dataware that we want to scan. 3. Register our Metric Packs & Rule Packs on Nexus and configure them into HDM . 4. Add an Airflow DAG to run them. 5. Run our HDM Airflow DAG and compute metrics/alerts . 6. Finally, add our Kibana Dashboards and use the Explorer and Alert Dashboard .","title":"HDM Full Installation"},{"location":"full-installation.html#1-launch-all-the-software-stack","text":"To run the stack we need : Docker See Get Docker Docker Compose See Get Docker Compose We are going to run the docker-compose files : - docker-compose.yml (HDM primary Stack) - docker-compose-airflow.yml (Airflow Stack) More INFO Here bash tutorials/full-installation/launch-stack.sh When the installation is complete, you should check the different application endpoints : http://localhost:80 HDM http://localhost:8081 Nexus http://localhost:5601 Kibana http://localhost:8080 Airflow (User: airflow | Password: airflow) tcp://127.0.0.1:3306 MySQL Endpoint host : 127.0.0.1 | Port : 3306 | User : hdm | Password : password | Database : dbhdm or: host : 127.0.0.1 | Port : 3306 | User : root | Password : rootpassword When you have all done. Let's go to the next step.","title":"1. Launch all the software stack"},{"location":"full-installation.html#2-ingest-a-dataset","text":"We are using the Kaggle API to download our example datasets.","title":"2. Ingest a Dataset"},{"location":"full-installation.html#21-kaggle-cli-installation","text":"In a Client with python 3 on it, run : pip install kaggle --upgrade","title":"2.1 Kaggle cli installation"},{"location":"full-installation.html#22-kaggle-cli-login","text":"Type kaggle to check if kaggle is installed. Setup API credentials : https://github.com/Kaggle/kaggle-api#api-credentials Run the commandline if needed : Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/<User>/.kaggle/kaggle.json' Test with : kaggle datasets list","title":"2.2 Kaggle cli login"},{"location":"full-installation.html#23-download-heart-attack-dataset","text":"kaggle datasets download rashikrahmanpritom/heart-attack-analysis-prediction-dataset -p ./datasets --unzip","title":"2.3 Download Heart Attack Dataset"},{"location":"full-installation.html#24-run-ingestion-script","text":"We are now going to ingest our Kaggle dataset to our MySQL database. bash tutorials/full-installation/ingest-data.sh Data is ingested ! Check it out on mysql://127.0.0.1:3306/heart-attack","title":"2.4 Run Ingestion Script"},{"location":"full-installation.html#3-metric-pack-rule-pack-registration","text":"In this step, we are going to register the metric pack and rule pack that are used for HDM.","title":"3. Metric Pack &amp; Rule Pack Registration"},{"location":"full-installation.html#31-setup-nexus","text":"In order to setup Nexus we need to get the password : docker exec -ti nexus sh -c \"cat /nexus-data/admin.password\" This will give you the admin password for Nexus Go to http://localhost:8081/ and login as \"admin\" + [Password from previous command] Do the setup by changing the default admin password and then checkout the [x][Enable Anonymous Access]","title":"3.1 Setup Nexus"},{"location":"full-installation.html#32-run-nexus-import-script","text":"\u26a0\ufe0f Change the PASSWORDNEXUS to your Nexus admin password value. # Nexus User Credentials export PASSWORDNEXUS = \"123qwe\" export USERNEXUS = \"admin\" And run the script : bash tutorials/full-installation/mp-rp-nexus-register.sh This script will create a Maven2 Repository on Nexus named : hdm-snapshots The script then packages into zip files the metric pack & rule pack basic and upload them into the maven repository. Check if it's ok : http://localhost:8081/#browse/browse:hdm-snapshots","title":"3.2 Run Nexus Import Script"},{"location":"full-installation.html#33-hdm-db-initialization","text":"We now have to initialize the hdm core db Go to the [Databases] Admin Tab http://localhost/admin.php?tab=databases Then click in this order on : [Launch db hdm script creator] [Sync db-config File to HDM's Table [Database List]]","title":"3.3. HDM DB Initialization"},{"location":"full-installation.html#34-metric-pack-rule-pack-configuration","text":"","title":"3.4. Metric Pack &amp; Rule Pack Configuration"},{"location":"full-installation.html#341-enabling-metric-pack-rule-pack","text":"We Then have to activate our mp & rp on : http://localhost/admin.php?tab=metricpacks http://localhost/admin.php?tab=rulepacks","title":"3.4.1 Enabling Metric Pack / Rule Pack"},{"location":"full-installation.html#342-edit-configuration-metric-pack-rule-pack","text":"We edit our metric pack configuration to add : { \"print_cat_var\" : false , \"print_mat_num_plot\" : false , \"limit_enabled\" : true , \"search_results_limit\" : 2000000 , \"rootResultFolder\" : \"../results/\" , \"esHost\" : \"elasticsearch\" , \"esPort\" : 9200 , \"esSSL\" : false } And same for our rule pack with : dev","title":"3.4.2 Edit Configuration Metric Pack / Rule Pack"},{"location":"full-installation.html#4-airflow-dag","text":"Login to Airflow http://localhost:8080/home with (login : airflow | password: airflow)","title":"4. Airflow DAG"},{"location":"full-installation.html#41-add-env-variables","text":"In your previous terminal run these commands : # Airflow User Credentials export PASSWORDAIRFLOW = \"airflow\" export USERAIRFLOW = \"airflow\" # Add variables curl -u $USERAIRFLOW : $PASSWORDAIRFLOW -X POST \"http://localhost:8080/api/v1/variables\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"key\\\":\\\"env\\\",\\\"value\\\":\\\"dev\\\"}\" They will create all the airflow environment variables in order for our DAG to run.","title":"4.1 Add env variables"},{"location":"full-installation.html#42-enable-the-dag","text":"Toggle the dag :","title":"4.2 Enable the dag"},{"location":"full-installation.html#5-run-the-dag","text":"Trigger the dag : You can check it's execution : http://localhost:8080/graph?dag_id=hdm-pipeline","title":"5. Run the dag"},{"location":"full-installation.html#6-hdm-visualisation","text":"","title":"6. HDM Visualisation"},{"location":"full-installation.html#61-import-kibana-dashboard","text":"Run the following comandline to import all the dashboards from the Basic Metric Pack into kibana. curl -X POST http://localhost:5601/api/saved_objects/_import?overwrite = true -H \"kbn-xsrf: true\" --form file = @packs/hdm-metric-packs/basic/kibana-dashboard/export.ndjson","title":"6.1 Import kibana dashboard"},{"location":"full-installation.html#62-explorer-dashboard","text":"You can explore the different metric pack dashboards from the Explorer. http://localhost/explorer/wrapper.php","title":"6.2 Explorer Dashboard"},{"location":"full-installation.html#63-alert-dashboard","text":"You can check all the alerts emmitted by the different rule packs from the Alert dashboard : http://localhost/alert/alert.php","title":"6.3 Alert Dashboard"},{"location":"full-installation.html#7-stopping-hdm-stack","text":"To stop the stack : docker-compose -f docker-compose.yml down docker-compose -f docker-compose-airflow.yaml down -v docker-compose down -v","title":"7. Stopping HDM Stack"},{"location":"packs.html","text":"HDM Packs Les Packs dans HDM sont des micro programmes autonomes qui permettent d'effectuer des traitements sur : Les bases de donn\u00e9es (Metric Packs) Les m\u00e9triques (Rule Packs) Il n'existe que deux types de packs HDM pour le moment.","title":"HDM Packs"},{"location":"packs.html#hdm-packs","text":"Les Packs dans HDM sont des micro programmes autonomes qui permettent d'effectuer des traitements sur : Les bases de donn\u00e9es (Metric Packs) Les m\u00e9triques (Rule Packs) Il n'existe que deux types de packs HDM pour le moment.","title":"HDM Packs"}]}