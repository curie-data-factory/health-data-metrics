{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-hdm","title":"What is HDM?","text":"<p>Health data metrics (HDM) is a tool developed by the Data Factory of Institut Curie.</p>"},{"location":"#the-goal","title":"The goal","text":"<p>Its goal is to be able to calculate quality metrics on medical data and its storage in data warehouses.</p> <ol> <li>The primary goal is to be able to improve and monitor the data quality of our health data warehouses.</li> </ol> <p>To do this we have developed the following features:</p> <ul> <li>Calculate metrics on the data from our warehouses.</li> <li>Set up rules to be able to apply operational / business constraints on the databases in connection with the calculated metrics.</li> <li>Detect breaks and regressions in the database structure or in the data itself by generating alerts using business rules.</li> <li>Allow to centralize the constraints and create a unified HUB to manage the quality of the data in order to deliver the best possible quality data to doctors and researchers.</li> <li>Create dashboards on metrics to be able to visualize and explore them.</li> </ul>"},{"location":"#the-basic-principles","title":"The basic principles","text":"<p>There is a hierarchical relationship in the way HDM works:</p> <ul> <li>First, we have databases on which we will calculate metrics</li> </ul> <p>These metrics can serve different purposes and are calculated using Metrics Packs which are self-contained mini programs that will calculate them and then insert them into a metrics database.</p> <ul> <li>Then we have the Rules Packs whose goal is to be able to use the metrics calculated previously in order to generate alerts that the user can consult.</li> </ul> <p>In the same way as for the metric Packs, the Rule Packs also work as autonomous mini programs which will take charge this time of generating rules based on the metrics and of generating alerts if ever these rules are not respected.</p>"},{"location":"#functional-architecture","title":"Functional architecture","text":"<p>HDM works in conjunction with several tools and technologies that we will see below :</p>    Logiciel Containerized Version Usage url     Elasticsearch Yes 7.10.0 Permet de stocker les m\u00e9triques sous forme de documents pour la construction des dashboards https://www.elastic.co/fr/products/elasticsearch   Kibana Yes 7.10.0 Permet de r\u00e9aliser des tableaux de visualisations interactives pour l'analyse exploratoire des m\u00e9triques https://www.elastic.co/fr/products/kibana   Python Yes 3.6.8 Permet d'ex\u00e9cuter les scripts pour le calcul des m\u00e9triques et la g\u00e9n\u00e9ration des alertes https://www.python.org/download/releases/3.0/   Apache Yes php:7.4.5-apache Serveur Web avec PHP 7 pour l'affichage de la web-Application https://hub.docker.com/   MySQL Yes 8.0.16 Base de donn\u00e9es relationnelle qui permet de stocker les m\u00e9triques, les r\u00e8gles et les alertes. https://www.mysql.com/fr/   Airflow Yes 2.1.0 Orchestrateur de T\u00e2ches. https://airflow.apache.org/"},{"location":"#the-components","title":"The components","text":""},{"location":"#data-base","title":"Data base","text":"<p>This tool integrates 3 databases:</p> <ul> <li>A MySQL database to store metrics in SQL format.</li> <li>A MySQL database to store application data. (conf, parameters, logs etc ...)</li> <li>An Elasticsearch database to store metrics in NO-SQL format.</li> </ul>"},{"location":"#front-end-modules","title":"Front-end modules","text":"<p>The front-end tool works in a modular way, there are the following modules:</p> <ul> <li>Explore (Dashboarding to explore metrics)</li> <li>Rule editor (Allows you to edit consistency rules directly in the interface without code lines)</li> <li>Alerts (Allows you to display the alerts sent by the Rule Packs)</li> <li>Admin (Used to manage the administration of MP / RP and their configuration)</li> </ul>"},{"location":"#dashboards","title":"Dashboards","text":"<p>This tool can be used with the following tools:</p> <ul> <li>Kibana which allows you to create views from documents in the NO-SQL Elasticsearch databases.</li> <li>Redash which allows you to create views from SQL MySQL metric tables.</li> </ul>"},{"location":"#orchestrator-jobs","title":"Orchestrator &amp; Jobs","text":"<p>In order to monitor the execution of metrics, rules and alerts calculation tasks, we use Airflow, through the definition of a DAG called: <code>hdm-pipeline</code>.</p>"},{"location":"#metrics-and-rules","title":"Metrics and rules","text":"<p>How are metrics and rules modules defined and calculated?</p> <p>In HDM there are what are called \"PACKs\", the packs are mini-modules which are responsible for calculating either metrics or data consistency rules.</p> <p>In the current version of HDM there is only one metric pack but it is possible to add others later without having to modify the application.</p>"},{"location":"#operation-of-metricpacks-mp","title":"Operation of \"MetricPacks\" (MP)","text":"<p>The metric packs contain a <code>.sh</code> file which contains the execution line of the MP, it is the responsibility of the MP programmer to do all the software dependency checks before running his program.</p> <p>The metricPack can also contain an <code>.ndjson</code> file containing the Kibana visualizations &amp; dashboards which are linked to the metrics data that its PM produces.</p>"},{"location":"#how-rulepacks-rp-work","title":"How RulePacks (RP) work","text":"<p>The rules packs contain a <code>.sh</code> file which contains the line of execution of the RP, it is the responsibility of the programmer of the MP to do all the software dependency checks before running his program.</p> <p>The rule packs are executed on the data of the PMs and add alerts in the alert table of HDM.</p>"},{"location":"#configuration-orchestration","title":"Configuration &amp; Orchestration","text":"<p>The configuration of the HDM application is internal to the application itself, there are the following configurations:</p> <ul> <li>LDAP configuration: Used to connect to the LDAP server, manage user authentication and manage their rights</li> <li>Application configuration: Which Kibana dashboards to display in which modules etc ...</li> <li>Packs configuration: Connection to the Packs directory (Nexus) General configuration of the packs</li> </ul> <p>Each MetricPack &amp; RulePack can have its own configuration. It is stored in base64 in the application database. Each MP &amp; RP can also have its own configuration per scanned database and can therefore adapt its operation to the target database if necessary. If this configuration exists, it will be loaded instead of the configuration of the parent MP / RP.</p> <p>The orchestration is performed by a DAG (Directed Acyclic Graph) Airflow which will load the MP / RP then load their configuration, and finally run them on each database.</p>"},{"location":"#data-quality-metrics","title":"Data quality metrics","text":""},{"location":"#metric-levels","title":"Metric levels","text":"<p>In order to correctly assess the quality of the data, it is good to define the calculation scopes of these metrics. We have defined 6 hierarchical levels of metrics; It is important not to mix the metric levels, and to compare the metrics on the same level.</p> <ul> <li>Level 0: <p>Level 0 is used to calculate metrics at the scale of all the versions of a database.</p>  </li> <li>Level 1: <p>Level 1 is used to calculate metrics at the scale of a given version of a database.</p>  </li> <li>Level 2: <p>Level 2 is used to calculate metrics at the scale of a table, a given version, or a database.</p>  </li> <li>Level 3: <p>Level 3 is used to calculate metrics at the scale of a column regardless of its type (example: number of missing values \u200b\u200b/ NULL values).</p>  </li> <li>Level 4: <p>Level 4 is used to calculate metrics at the scale of a column by taking into account its type of data (Numeric, textual, categorical, continuous, date, id etc. ...).</p>  </li> <li>Level 5: <p>Level 5 is used to calculate metrics on the scale of categorical variables: (frequency, value)</p>  </li> </ul>   <p>Diagram showing the different levels of metric calculation possible.</p>"},{"location":"#rules-alerts","title":"Rules &amp; alerts","text":""},{"location":"#the-rules","title":"The rules","text":"<p>Based on the metrics calculated periodically, the application makes it possible to ensure passive (without human intervention) and dynamic (adapts to changes) monitoring of the quality of the data, using the implementation tool of alert rules in order to report anomalies in the database as efficiently as possible.</p>   <p>Display of the rule editor graphical interface.</p>"},{"location":"#alerts","title":"Alerts","text":"<p>Alerts will be presented as a simple list and organized by database, table or columns.</p>   <p>Display of the alerts table.</p>"},{"location":"#visualization-and-exploration","title":"Visualization and exploration","text":"<p>The visualization of metrics and their exploration is an important phase in order to build the most relevant rules possible. A tool is available called Kibana, which is based on the Elasticsearch database. The dashboards will make it possible to visualize and simply interact with the metrics in order to be able to easily detect outliers, and to be able to raise these anomalies in alert thanks to the rule editor.</p>   <p>Display of the kibana dashboard graphical interface.</p>"},{"location":"full-installation/","title":"HDM Full Installation","text":"<p>This Tutorial guides you on How to install hdm full stack on your local machine.</p> <p>\u26a0\ufe0f     REQUIREMENTS \u26a0\ufe0f</p> <p>Software :</p> <ul> <li>Linux/MacOS 64bit or Windows 10 64bit with WSL2</li> <li>Docker</li> <li>Docker-compose</li> <li>Python 3.9+</li> </ul> <p>Hardware : Minimal\ud83e\udd13</p>  <pre><code>- CPU     :   4 Cores\n- RAM     :  16 Go\n- Storage :  10 Go\n</code></pre>  <p>Hardware : Recommended  \ud83d\ude0e</p>  <pre><code>- CPU     :  12 Cores\n- RAM     :  32 Go\n- Storage :  30 Go\n</code></pre>  <p>In this Tutorial, we are going to install HDM in Full Stack mode. That means that we are going to :</p>  <p>\u26a0\ufe0f Before we start : all comandlines have to be executed at the root folder of the git source repository</p>  <p>1. Launch all the software stack :</p> <pre><code>- Airflow\n- Nexus\n- Elasticsearch\n- Kibana\n- MySQL\n- HDM frontend\n</code></pre> <p>2. Ingest some dataset to our MySQL database, simulating a dataware that we want to scan.</p> <p>3. Register our Metric Packs &amp; Rule Packs on Nexus and configure them into HDM.</p> <p>4. Add an Airflow DAG to run them.</p> <p>5. Run our HDM Airflow DAG and compute metrics/alerts.</p> <p>6. Finally, add our Kibana Dashboards and use the Explorer and Alert Dashboard.</p>"},{"location":"full-installation/#1-launch-all-the-software-stack","title":"1. Launch all the software stack","text":"<p>To run the stack we need :</p> <ul> <li><code>Docker</code> See Get Docker</li> <li><code>Docker Compose</code> See Get Docker Compose</li> </ul> <p>We are going to run the <code>docker-compose</code> files : - <code>docker-compose.yml</code> (HDM primary Stack) - <code>docker-compose-airflow.yml</code> (Airflow Stack) More INFO Here</p> <pre><code>sed -i -e 's/\\r$//' tutorials/full-installation/*.sh\nsed -i -e 's/\\r$//' packs/hdm-metric-packs/basic/*.sh\nsed -i -e 's/\\r$//' packs/hdm-rule-packs/basic/*.sh\nbash tutorials/full-installation/launch-stack.sh\n</code></pre> <p>When the installation is complete, you should check the different application endpoints :</p> <ul> <li>http://localhost:80 HDM</li> <li>http://localhost:8081 Nexus</li> <li>http://localhost:5601 Kibana</li> <li>http://localhost:8080 Airflow (User: airflow | Password: airflow)</li> <li>tcp://127.0.0.1:3306 MySQL Endpoint <p>host: 127.0.0.1 | Port: 3306 | User: hdm | Password: password | Database: dbhdm</p>  </li> </ul> <p>or:</p>  <p>host: 127.0.0.1 | Port: 3306 | User: root | Password: rootpassword</p>  <p>When you have all done. Let's go to the next step.</p>"},{"location":"full-installation/#2-ingest-a-dataset","title":"2. Ingest a Dataset","text":"<p>We are using the Kaggle API to download our example datasets.</p>"},{"location":"full-installation/#21-kaggle-cli-installation","title":"2.1 Kaggle cli installation","text":"<p>In a Client with python 3 on it, run :</p> <pre><code>pip install kaggle --upgrade\n</code></pre>"},{"location":"full-installation/#22-kaggle-cli-login","title":"2.2 Kaggle cli login","text":"<p>Type <code>kaggle</code> to check if kaggle is installed.</p> <p>Setup API credentials :  https://github.com/Kaggle/kaggle-api#api-credentials</p> <p>Run the commandline if needed :</p>  <pre><code>Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/&lt;User&gt;/.kaggle/kaggle.json'\n</code></pre>  <p>Test with : <code>kaggle datasets list</code></p>"},{"location":"full-installation/#23-download-heart-attack-dataset","title":"2.3 Download Heart Attack Dataset","text":"<pre><code>kaggle datasets download rashikrahmanpritom/heart-attack-analysis-prediction-dataset -p ./datasets --unzip\n</code></pre>"},{"location":"full-installation/#24-run-ingestion-script","title":"2.4 Run Ingestion Script","text":"<p>We are now going to ingest our Kaggle dataset to our MySQL database.</p> <pre><code>bash tutorials/full-installation/ingest-data.sh\n</code></pre> <p>Data is ingested ! Check it out on mysql://127.0.0.1:3306/heart-attack</p>"},{"location":"full-installation/#3-metric-pack-rule-pack-registration","title":"3. Metric Pack &amp; Rule Pack Registration","text":"<p>In this step, we are going to register the metric pack and rule pack that are used for HDM.</p>"},{"location":"full-installation/#31-setup-nexus","title":"3.1 Setup Nexus","text":"<ol> <li>In order to setup Nexus we need to get the password :</li> </ol> <pre><code>docker exec -ti nexus sh -c \"cat /nexus-data/admin.password\"\n</code></pre> <p>This will give you the admin password for Nexus</p> <ol> <li> <p>Go to http://localhost:8081/ and login as \"admin\" + [Password from previous command]</p> </li> <li> <p>Do the setup by changing the default admin password and then checkout the [x][Enable Anonymous Access]</p> </li> </ol>"},{"location":"full-installation/#32-run-nexus-import-script","title":"3.2 Run Nexus Import Script","text":"<p>\u26a0\ufe0f Change the PASSWORDNEXUS to your Nexus admin password value.</p> <pre><code># Nexus User Credentials\nexport PASSWORDNEXUS=\"123qwe\"\nexport USERNEXUS=\"admin\"\n</code></pre> <p>And run the script :</p> <pre><code>bash tutorials/full-installation/mp-rp-nexus-register.sh\n</code></pre> <p>This script will create a Maven2 Repository on Nexus named : hdm-snapshots</p> <p>The script then packages into zip files the metric pack &amp; rule pack basic and upload them into the maven repository.</p> <p>Check if it's ok : http://localhost:8081/#browse/browse:hdm-snapshots</p>"},{"location":"full-installation/#33-hdm-db-initialization","title":"3.3. HDM DB Initialization","text":"<p>We now have to initialize the hdm core db Go to the [Databases] Admin Tab http://localhost/admin.php?tab=databases</p> <p>Then click in this order on :</p> <ol> <li>[Launch db hdm script creator]</li> <li>[Sync db-config File to HDM's Table [Database List]]</li> </ol>"},{"location":"full-installation/#34-metric-pack-rule-pack-configuration","title":"3.4. Metric Pack &amp; Rule Pack Configuration","text":""},{"location":"full-installation/#341-enabling-metric-pack-rule-pack","title":"3.4.1 Enabling Metric Pack / Rule Pack","text":"<p>We Then have to activate our mp &amp; rp on :</p> <ul> <li>http://localhost/admin.php?tab=metricpacks</li> <li>http://localhost/admin.php?tab=rulepacks</li> </ul>"},{"location":"full-installation/#342-edit-configuration-metric-pack-rule-pack","title":"3.4.2 Edit Configuration Metric Pack / Rule Pack","text":"<p>We edit our metric pack configuration to add :</p> <p><pre><code>{\n  \"print_cat_var\": false,\n  \"print_mat_num_plot\": false,\n  \"limit_enabled\": true,\n  \"search_results_limit\": 2000000,\n  \"rootResultFolder\": \"../results/\",\n  \"esHost\": \"elasticsearch\",\n  \"esPort\": 9200,\n  \"esSSL\": false\n}\n</code></pre> </p> <p>And same for our rule pack with :</p> <pre><code>dev\n</code></pre>"},{"location":"full-installation/#4-airflow-dag","title":"4. Airflow DAG","text":"<p>Login to Airflow http://localhost:8080/home with (login : airflow | password: airflow)</p>"},{"location":"full-installation/#41-add-env-variables","title":"4.1 Add env variables","text":"<p>In your previous terminal run these commands :</p> <pre><code># Airflow User Credentials\nexport PASSWORDAIRFLOW=\"airflow\"\nexport USERAIRFLOW=\"airflow\"\n\n# Add variables\ncurl -u $USERAIRFLOW:$PASSWORDAIRFLOW -X POST \"http://localhost:8080/api/v1/variables\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"key\\\":\\\"env\\\",\\\"value\\\":\\\"dev\\\"}\"\n</code></pre> <p>They will create all the airflow environment variables in order for our DAG to run.</p>"},{"location":"full-installation/#42-enable-the-dag","title":"4.2 Enable the dag","text":"<p>Toggle the dag :</p>"},{"location":"full-installation/#5-run-the-dag","title":"5. Run the dag","text":"<p>Trigger the dag :</p>  <p>You can check it's execution :</p> <p>http://localhost:8080/graph?dag_id=hdm-pipeline</p>"},{"location":"full-installation/#6-hdm-visualisation","title":"6. HDM Visualisation","text":""},{"location":"full-installation/#61-import-kibana-dashboard","title":"6.1 Import kibana dashboard","text":"<p>Run the following comandline to import all the dashboards from the Basic Metric Pack into kibana.</p> <pre><code>curl -X POST http://localhost:5601/api/saved_objects/_import?overwrite=true -H \"kbn-xsrf: true\" --form file=@packs/hdm-metric-packs/basic/kibana-dashboard/export.ndjson\n</code></pre>"},{"location":"full-installation/#62-explorer-dashboard","title":"6.2 Explorer Dashboard","text":"<p>You can explore the different metric pack dashboards from the Explorer.</p> <p>http://localhost/explorer/wrapper.php</p>"},{"location":"full-installation/#63-alert-dashboard","title":"6.3 Alert Dashboard","text":"<p>You can check all the alerts emmitted by the different rule packs from the Alert dashboard :</p> <p>http://localhost/alert/alert.php</p>"},{"location":"full-installation/#7-stopping-hdm-stack","title":"7. Stopping HDM Stack","text":"<p>To stop the stack :</p> <pre><code>docker-compose -f docker-compose.yml down\ndocker-compose -f docker-compose-airflow.yaml down -v\ndocker-compose down -v\n</code></pre>"},{"location":"packs/","title":"HDM Packs","text":"<p>Packs in HDM are autonomous micro programs which allow processing on :</p> <ul> <li>Databases (Metric Packs)</li> <li>Metrics (Rule Packs)</li> </ul> <p>There are only two types of HDM packs at the moment.</p>"},{"location":"packs/#tree-structure","title":"Tree structure","text":"<p>The package tree must meet a standard in order to be able to build automated systems and services in the future.</p>    File Description     Readme.md Used to explain how the pack works   requirements.txt package software dependencies   properties.json Properties.json   bootstrap-script.sh bootstrap-script.sh   /process The process folder contains the program as such   /process/conf.json the conf.json file contains the minimum local configuration when the pack is run   /process/code.(py,jar,etc...) the program code in any format (no constraints)   /kibana-dashboard If necessary (for metric packs) this folder will contain the kibana / elasticsearch files to install the visualizations/dashboards of the pack   /kibana-dashboard/export.ndjson the kibana visualizations file   /create-table The folder which contains the SQL script (s) necessary for the operation of the pack   /create-table/create.sql sql file"},{"location":"packs/#propertiesjson","title":"Properties.json","text":"<p>The package properties.json file should contain basic information to allow listing in the future.</p> <ul> <li>name : Name of the pack</li> <li>version : Version of the pack</li> <li>author : author</li> <li>description : One line description</li> <li>mainscript : main, but can be in the <code>.sh</code> also</li> </ul> <pre><code>{\n    \"name\":\"basic\",\n    \"version\":\"0.3.7\",\n    \"author\":\"armand leopold\",\n    \"description\": \"Ce pack contient des m\u00e9triques de base ainsi que l'analyse du delta d'un jour a l'autre des m\u00eames metriques, et ajoute un coefficient de leur variation.\",\n    \"mainscript\":\"metrics.py\"\n}\n</code></pre>"},{"location":"packs/#bootstrap-scriptsh","title":"Bootstrap-script.sh","text":"<p>The bootstrap script file is a necessary file that serves as an entry point for the program that is going to be executed. If necessary, we will include the installation of dependencies and binaries required for the proper execution of the program. Finally, we will include the command to launch the program (s) of the pack.</p> <pre><code>#!/bin/bash\n# This script will be executed first at the installation of the pack\n# Put Here everything that needs to be installed in order for the metric script to run properly\n\npip install --upgrade pip \npip install -r requirements.txt\ncd process\npython metric_basic.py\n</code></pre>"},{"location":"packs/#configuration-management","title":"Configuration management","text":"<p>The configuration management is left free to the author of the pack for the most part, except in the following cases:</p> <ol> <li>Retrieving the list of endpoints on which the pack must run.</li> <li>The specific configuration for each endpoint</li> <li>The external configuration of the pack (excluding credentials) will change regularly.</li> </ol>"},{"location":"packs/#1-list-of-target-endpoints","title":"1. List of target endpoints","text":"<p>What is not meant by \"target endpoints\" are the set of connection chains or URLs which the Pack accesses occasionally to perform its work and against which it reads and retrieves information.</p> <p>This list of endpoints is managed centrally by HDM in order to have an overview at a glance from the interface, of the different actions of the packs.</p> <p>The list of these endpoints can be modified via the administration interface by clicking on the boxes in the activation matrix:</p>  <p>Each time an endpoint is activated, a row is added to the table :</p> <ul> <li><code>hdm_core_table_corr_db_mp</code> for Metric Packs</li> <li><code>hdm_core_table_corr_db_rp</code> for Rule Packs</li> </ul> <p>Containing the slug of the connection string in the <code>db_key</code> field as well as the name of the pack in:<code>mp_key</code> or <code>rp_key</code> for a metric pack or a rule pack.</p> <p>Each pack must be able to connect to the HDM base to retrieve this configuration.</p>  <p>Warning</p> <p>Please note, the chain slug does not contain credentials. You will need to find another way to retrieve the connection string password. By using an online credentials store like Vault or Keycloack. You can also store credentials in the local pack configuration as a last resort and only for \"readonly\" accounts.</p>"},{"location":"packs/#23-configuration-of-termination-points-and-external-configuration-of-packs","title":"2./3. Configuration of termination points and external configuration of packs","text":"<p>HDM also gives the possibility of centralizing the configuration specific to each endpoint for each pack.</p> <p>In the interface, the configuration is also located in the activation matrix (see image above) to the right of the activation button.</p> <p>The configuration is then encoded in base64 and stored in the table:</p> <ul> <li><code>hdm_pack_metric_conf</code> for Metric Packs</li> <li><code>hdm_pack_rule_conf</code> for Rule Packs</li> </ul> <p>These tables also contain <code>the external configuration of the pack</code> the subtlety is as follows:</p> <p>The <code>config_id</code> contains either:</p> <ul> <li>The pack id + version: pack: version if it is an external configuration.</li> <li>The pack id + the connection string slug: pack: slug if it is a configuration linked to an endpoint.</li> </ul> <p>This table is also to be requested by the pack to retrieve its configuration and the endpoint configurations.</p>"},{"location":"packs/#pack-orchestration","title":"Pack orchestration","text":"<p>For pack orchestration, the proposed solution is to use an Airflow DAG. The dag airflow makes it possible to centralize the execution of packs as \"tasks\", to configure an execution rate, to monitor the tasks executed and the correct functioning of the sequences of tasks.</p> <p>Indeed, the packs functioning independently of one another may nevertheless require resources from other packs to function. For example, rule packs need metric packs to execute their rules. Airflow, because of its execution graph allows to ensure a correct sequence of packs one after the other in a defined order, it allows to avoid any corruption in the analyzes and prevents the emission of false alerts.</p>"}]}